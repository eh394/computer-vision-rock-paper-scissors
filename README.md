# Computer Vision RPS

# MILESTONE 2
Currently two files have been uploaded onto github, keras_model.h5 and labels.txt. The first file contains machine learning outputs from Teachable Machine (TM) by Google, a web-based tool that facilitates creating machine learning tools in an accessible manner. Using TM, four different classes of data were created: Rock, Paper, Scissors and Nothing. Rock, Paper and Scissors all correspond to symbols of the old childhood gesture game. Class Nothing corresponds to no gesture. Each of the classes have been given a series of inputs, approx. 3,000 image samples per class. These inputs were then put through a 'training' process to allow future recognition of these inputs based on this data sample. The model certainly has its (severe) limitations in that it is largely based on one face and use of a right hand. Much larger data sample would be required to create a model with higher accuracy, however, data collected here is considered sufficient for the purpose of this project.
